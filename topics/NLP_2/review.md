**ğŸ§  Foundations of Sequence Modeling**
---

### ğŸŒŸ Concept Overview

**Goal:** Model sequential dependencies in text/speech (order matters!)
**Shift:** From **count-based (n-gram)** to **learned neural sequence models (RNNs, LSTMs, etc.)**

---

### ğŸ”¹ 1. Traditional Models â€” n-gram Approach

* **Idea:** Predict next word using previous (nâ€“1) words
  â†’ P(wâ‚œ | wâ‚œâ‚‹â‚, wâ‚œâ‚‹â‚‚, â€¦, wâ‚œâ‚‹â‚™â‚Šâ‚)
* **Pros:** Simple, interpretable
* **Cons:**

  * Fixed-length context (no long-term dependencies)
  * Data sparsity â†’ unseen n-grams have zero probability
  * Need smoothing (Laplace, Kneser-Ney)
* **Transition Motivation:** Fails to scale for long sequences â†’ Neural sequence models introduced

---

### ğŸ”¹ 2. Neural Sequence Models â€” RNN Family

**Key Concept:** Hidden state captures â€œmemoryâ€ of previous inputs

#### ğŸ§© Vanilla RNN

* **Equation:** hâ‚œ = f(Wâ‚•â‚“xâ‚œ + Wâ‚•â‚•hâ‚œâ‚‹â‚ + bâ‚•)
* **Challenge:** Vanishing/exploding gradients

  * Long sequences â†’ earlier timesteps have negligible influence

#### âš™ï¸ Deep & Bi-Directional RNNs

* **Deep RNN:** Multiple stacked RNN layers â†’ hierarchical features
* **Bi-Directional RNN:**

  * Forward + backward context
  * Useful for tasks needing both past and future info (e.g. NER)

#### ğŸ”„ GRU (Gated Recurrent Unit)

* **Simplified LSTM:** Combines forget & input gates into **update gate**
* **Fewer parameters**, faster training

#### ğŸ§  LSTM (Long Short-Term Memory)

* **Solves vanishing gradient problem** via **cell state (Câ‚œ)**
* Gates:

  * **Forget gate (fâ‚œ)** â†’ What to remove
  * **Input gate (iâ‚œ)** â†’ What to add
  * **Output gate (oâ‚œ)** â†’ What to expose
* Enables **long-term dependency capture**

---

### ğŸ”¹ 3. RNNs and the Vanishing Gradient Problem

* **Reason:** Repeated multiplication of small weights < 1
* **Effect:** Early timesteps lose gradient contribution â†’ memory fades
* **Fixes:**

  * Use gated units (LSTM/GRU)
  * Gradient clipping
  * Layer normalization
  * Residual connections in deeper RNNs

---

### ğŸ”¹ 4. Sequence Model vs Attention Model

| Aspect                  | Sequence (RNN-based)              | Attention-based           |
| :---------------------- | :-------------------------------- | :------------------------ |
| **Dependency modeling** | Sequential (one step at a time)   | Parallel (global context) |
| **Memory capacity**     | Limited (hidden state bottleneck) | Full access to all tokens |
| **Speed**               | Slow (non-parallel)               | Fast (parallelizable)     |
| **Examples**            | LSTM, GRU                         | Transformer, BERT, GPT    |

**Key Insight:**
RNNs encode context into a single hidden vector â†’ information bottleneck.
Attention lets the model **directly â€œattendâ€** to all past tokens simultaneously.

---

### ğŸ”¹ 5. Application Highlight â€” Named Entity Recognition (NER)

* **Goal:** Identify entities like *person*, *location*, *organization*
* **Model Setup:**

  * Input: sequence of words
  * Output: label per word (BIO tagging: B = Begin, I = Inside, O = Outside)
* **Typical Model:**

  * Bi-LSTM (captures both directions)
  * * CRF layer (Conditional Random Field) for sequence-level labeling consistency
* **Evaluation Metrics:** Precision, Recall, F1-score

---

### ğŸ”¹ 6. Key Takeaways

âœ… Neural sequence models overcome fixed-context limits of n-grams
âœ… RNNs introduced recurrence â†’ memory of past inputs
âœ… LSTMs/GRUs solve vanishing gradients via gating mechanisms
âœ… Bi-directional models help in context-rich tasks (e.g., NER)
âœ… Attention models supersede RNNs in efficiency and global context modeling

---

**ğŸ§­ Quick Mnemonics:**

* **N-gram â†’ RNN â†’ LSTM â†’ Attention â†’ Transformer**
* **Gates (LSTM):** Forget whatâ€™s unnecessary, Input new info, Output meaningful context.
* **NER:** Bi-LSTM + CRF = context-aware sequence labeling

---
**âš¡ Attention Mechanisms & Transition to Transformers**
*(The revolution that replaced recurrence with attention and gave rise to Transformers)*

---

### ğŸŒ Concept Overview

**Goal:** Enable models to capture **long-range dependencies** and **contextual relationships** *without recurrence*.
**Breakthrough:** Attention allows **direct access** to all input positions simultaneously â†’ faster, more global understanding.

---

### ğŸ”¹ 1. The Attention Concept

**Core Idea:** Let the model **â€œfocusâ€** on relevant parts of the input sequence when producing an output.

* Each output token is computed as a **weighted sum** of all input tokens.
* Weights (attention scores) indicate **importance**.

#### ğŸ§® Scaled Dot-Product Attention

Formula:
**Attention(Q, K, V) = softmax(QKáµ€ / âˆšdâ‚–) Ã— V**

* **Q (Query):** What weâ€™re looking for (current token)
* **K (Key):** What each input token represents
* **V (Value):** The information each token holds
* **Scaling by âˆšdâ‚–:** Prevents large dot products from saturating softmax
* **Intuition:** Similarity(Q, K) â†’ attention strength

---

### ğŸ”¹ 2. Types of Attention

Different variants for different purposes in model architectures:

#### ğŸ”¸ Self-Attention

* **Query, Key, Value come from the same sequence.**
* Each token attends to **all tokens (including itself)**.
* Captures context relationships within a sentence.
* Example: *â€œThe animal didnâ€™t cross because it was too tired.â€* â†’ â€œitâ€ attends to â€œanimal.â€

#### ğŸ”¸ Cross-Attention

* Used in **Encoderâ€“Decoder** models (e.g., Translation).
* **Query** = decoderâ€™s current hidden state
* **Key/Value** = encoder outputs
* Lets the decoder â€œlook backâ€ at the source sequence.

#### ğŸ”¸ Masked Self-Attention

* Used in **autoregressive models** (e.g., GPT).
* Prevents looking ahead â†’ ensures causal (left-to-right) prediction.

#### ğŸ”¸ Multi-Head Attention

* Multiple attention heads = multiple representation subspaces.
* Each head learns different relationships (syntax, semantics, etc.).
* Final outputs are concatenated and projected â†’ richer context representation.

**Analogy:** Like multiple readers focusing on different parts of the same paragraph.

---

### ğŸ”¹ 3. Position Encoding

**Problem:** Attention has no notion of word order (no recurrence).
**Solution:** Add **positional information** to token embeddings.

* **Sinusoidal Encoding:**

  * Fixed patterns (sin & cos) of different frequencies
  * Allows model to infer relative positions via continuous signals
* **Learned Positional Embeddings:**

  * Parameters learned during training (e.g., BERT)

**Intuition:**
Encodes *â€œwhereâ€* each token is in sequence â†’ gives attention models a sense of order.

---

### ğŸ”¹ 4. RNNs vs Transformers

| Aspect                | RNNs                             | Transformers                       |
| :-------------------- | :------------------------------- | :--------------------------------- |
| **Processing**        | Sequential (one token at a time) | Parallel (entire sequence at once) |
| **Memory**            | Hidden state bottleneck          | Global context via attention       |
| **Long Dependencies** | Hard to capture                  | Easily modeled                     |
| **Training Time**     | Slow                             | Fast (GPU-friendly)                |
| **Architecture**      | Recurrent layers                 | Stacked attention blocks           |

**Core Transition:**
Transformers removed recurrence â†’ enabling parallelism, scalability, and massive pretraining.

---

### ğŸ”¹ 5. Transformer Architecture Overview

**Encoderâ€“Decoder Design** (e.g., in original *Vaswani et al., 2017*):

* **Encoder:** Stack of self-attention + feed-forward layers
* **Decoder:** Self-attention + cross-attention + feed-forward layers
* **Residual connections + LayerNorm** â†’ stabilize training

---

### ğŸ”¹ 6. Landmark Transformer Models

**ğŸš€ GPT (Generative Pre-trained Transformer)**

* **Type:** Decoder-only
* **Objective:** Next-token prediction (causal LM)
* **Attention:** Masked self-attention
* **Use:** Text generation, chatbots, code models (GPT-4, GPT-5)

**ğŸ§© BERT (Bidirectional Encoder Representations from Transformers)**

* **Type:** Encoder-only
* **Objective:** Masked Language Modeling (MLM) + Next Sentence Prediction (NSP)
* **Attention:** Bidirectional self-attention
* **Use:** Text understanding, classification, QA

**ğŸ”„ T5 (Text-to-Text Transfer Transformer)**

* **Type:** Full Encoderâ€“Decoder
* **Objective:** Unified text-to-text task framing (â€œtranslate everything to textâ€)
* **Use:** Summarization, translation, question answering

---

### ğŸ”¹ 7. Key Insights & Review Notes

âœ… Attention replaces recurrence with *direct relevance weighting*
âœ… Multi-head attention learns diverse context relationships
âœ… Positional encoding gives order sense
âœ… Transformers scale efficiently with parallelization
âœ… GPT = Generation | BERT = Understanding | T5 = Unified Text2Text
âœ… RNNs = sequential, Transformers = fully parallel

---

### ğŸ§­ Quick Mnemonics

* **â€œQKV Ruleâ€ â†’ Query, Key, Value â†’ Attention core.**
* **â€œSelfâ€ = same sequence | â€œCrossâ€ = between encoder & decoder.**
* **â€œMaskâ€ = future-blocking for prediction.**
* **â€œMulti-headâ€ = multiple perspectives.**
* **â€œTransformersâ€ = attention + position + feed-forward.**

---
**ğŸ¯ Advanced Training & Decoding Techniques**
*(Training tricks, decoding strategies, and evaluation metrics that shape sequence-to-sequence NLP systems)*

---

### ğŸŒ Concept Overview

**Goal:** Improve **training stability**, **generation quality**, and **evaluation reliability** in sequence models (especially Neural Machine Translation & Text Generation).
**Scope:** Covers how models learn (training dynamics), how they generate text (decoding), and how we measure their quality (evaluation).

---

### ğŸ”¹ 1. Neural Machine Translation (NMT) with Attention

**Core Idea:** Translate a source sequence into a target sequence using an **encoderâ€“decoder** setup guided by **attention**.

* **Encoder:** Encodes source tokens â†’ hidden representations
* **Attention Mechanism:** Learns alignment weights between source & target tokens
* **Decoder:** Generates target tokens while â€œattendingâ€ to relevant source words

**Advantages over vanilla seq2seq:**
âœ… Handles long sequences better
âœ… Dynamic focus on relevant words per step
âœ… Enables interpretable alignment visualization

---

### ğŸ”¹ 2. Pre-Attention vs Post-Attention Decoding

**ğŸ§© Pre-Attention Decoding**

* Decoder uses attention **before** combining with previous hidden state.
* Focuses early on context â†’ helpful for early alignment decisions.

**ğŸ”„ Post-Attention Decoding**

* Decoder first computes hidden state, then applies attention.
* Context integrated later â†’ allows more refined, top-down context control.

**ğŸ’¡ Insight:**
Both are architectural design choices in encoder-decoder models; they influence **information flow** and **training stability**.

---

### ğŸ”¹ 3. Teacher Forcing

**Definition:** During training, feed the **ground-truth token** as the next input instead of the modelâ€™s prediction.

**Pros:**
âœ… Speeds up convergence
âœ… Stabilizes early training

**Cons:**
âŒ Exposure bias â€” model never learns to recover from its own mistakes at inference time.

**Fixes / Alternatives:**

* **Scheduled Sampling:** Gradually replace true tokens with model predictions
* **Professor Forcing:** Regularizes hidden dynamics between training & inference

---

### ğŸ”¹ 4. Decoding Strategies â€” How Models Generate Text

Different methods to **sample** or **select** the next token during inference.

#### ğŸ”¸ Random Sampling

* Picks tokens based on probability distribution (purely random).
* High diversity, low coherence.

#### ğŸ”¸ Greedy Decoding

* Always pick the token with the highest probability (argmax).
* Simple but often **repetitive** and lacks diversity.

#### ğŸ”¸ Temperature Sampling

* Adjusts â€œcreativityâ€ by scaling logits before softmax:

  * **Low T (<1):** More deterministic, sharper distribution
  * **High T (>1):** More diverse, flatter distribution

#### ğŸ”¸ Beam Search

* Keeps **k** best partial sequences (beams) at each step.
* Expands them until completion, picks best-scoring final sequence.
* Balances between greedy (k=1) and exhaustive search.

#### ğŸ”¸ Top-k Sampling

* Restricts choices to the **top-k** most probable tokens, renormalizes distribution.
* Prevents extremely low-probability tokens.

#### ğŸ”¸ Top-p (Nucleus) Sampling

* Chooses smallest token set whose cumulative probability â‰¥ **p** (e.g., 0.9).
* Adapts cutoff dynamically based on distribution shape.

**Hierarchy of Control:**
Greedy < Beam < Top-k < Top-p < Random (increasing diversity, decreasing determinism)

---

### ğŸ”¹ 5. Problems with Beam Search

âš ï¸ Common failure modes in text generation:

* **Lack of diversity:** All beams converge to similar outputs.
* **Length bias:** Prefers shorter sequences (due to cumulative probability drop).
* **Overconfidence:** Amplifies small early mistakes.
* **Solution Approaches:**

  * Length normalization
  * Diverse beam search
  * Penalizing repeated n-grams

---

### ğŸ”¹ 6. Minimum Bayes Risk (MBR) Decoding

**Goal:** Select output minimizing **expected loss** rather than maximizing probability.

**Formula:**
Å· = argmin_yâ€² E_y[L(y, yâ€²)]
â†’ choose the output most similar to *many good hypotheses* instead of the single highest-probability one.

**Benefits:**
âœ… Improves faithfulness and robustness
âœ… Reduces beam search overconfidence
âœ… Aligns better with human evaluation metrics (BLEU/ROUGE)

---

### ğŸ”¹ 7. Evaluation Metrics â€” Measuring Text Quality

#### ğŸ§  BLEU (Bilingual Evaluation Understudy)

* **Used for:** Machine Translation
* **Measures:** n-gram **precision** (how many predicted n-grams appear in reference)
* **Formula:** Geometric mean of n-gram precisions Ã— brevity penalty
* **Weakness:** Penalizes valid paraphrases, favors shorter outputs

#### ğŸ§© ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

* **Used for:** Summarization, text generation
* **Measures:** n-gram **recall** (how much of reference text is captured)
* **Variants:**

  * ROUGE-N (n-gram recall)
  * ROUGE-L (Longest Common Subsequence)
  * ROUGE-W (Weighted)

**Comparison:**

| Metric    | Focus     | Best For      |                                      |
| :-------- | :-------- | :------------ | :----------------------------------- |
| **BLEU**  | Precision | Translation   | How accurate are modelâ€™s words?      |
| **ROUGE** | Recall    | Summarization | How much reference content captured? |

---

### ğŸ”¹ 8. Key Takeaways

âœ… Attention-guided decoding improves translation quality
âœ… Teacher forcing stabilizes training but causes exposure bias
âœ… Sampling controls creativity; beam search controls optimality
âœ… MBR aligns decoding with true objective metrics
âœ… BLEU & ROUGE remain standard automated evaluation metrics

---

### ğŸ§­ Quick Mnemonics

* **â€œTrain â†’ Decode â†’ Evaluateâ€** = core pipeline
* **Teacher forcing** = training shortcut
* **Temperature** = creativity knob
* **Beam width** = exploration scope
* **MBR** = accuracy vs similarity tradeoff
* **BLEU (Precision)**, **ROUGE (Recall)** = evaluate both sides

---

**ğŸ§© Fine-Tuning, Representation, and Evaluation Frameworks**
*(Model adaptation, representation learning, and evaluation in modern NLP)*

---

### ğŸŒ Concept Overview

**Goal:** Adapt pre-trained language models (PLMs) efficiently to downstream tasks while maintaining generalization and stability.
**Focus:**

* How to represent & compare sentence meanings
* How to fine-tune large models effectively
* How to evaluate semantic understanding in a standardized way

---

### ğŸ”¹ 1. Representation Learning & Semantic Similarity

Understanding how to represent text so that *semantic meaning* â€” not just surface form â€” is captured.

#### ğŸ§  Siamese Networks

**Concept:** Twin networks with shared weights â†’ learn *comparable embeddings* for two inputs.
**Pipeline:**

* Input: Sentence A, Sentence B
* Both passed through the same encoder (e.g., BERT, LSTM)
* Outputs â†’ fixed-size embeddings
* Compute **similarity score** (cosine / Euclidean)

**Used in:**

* Semantic Textual Similarity (STS)
* Sentence matching (e.g., paraphrase detection, duplicate questions)

**Advantages:**
âœ… Shared weights ensure consistent representation space
âœ… Works well with few-shot learning setups

---

### ğŸ”¹ 2. Triplet Loss â€” Learning Discriminative Embeddings

**Purpose:** Encourage semantically similar sentences to be close, and dissimilar ones to be far apart in embedding space.

**Triplet:**

* **Anchor (A):** reference sentence
* **Positive (P):** semantically similar sentence
* **Negative (N):** semantically different sentence

**Loss Function:**
L = max(0, d(A, P) - d(A, N) + margin)

**Key Variants:**

* **Mean Negative:** Average distance over multiple negatives
* **Closest Negative:** Choose hardest negative (smallest distance)

**Trade-off:**

* Mean negative â†’ stable training
* Closest negative â†’ faster convergence but risk of instability

**Used in:**
Sentence-BERT (SBERT), text retrieval, face recognition-style embedding tasks.

---

### ğŸ”¹ 3. Language Modeling Objectives for Fine-Tuning

#### ğŸ§© Masked Language Modeling (MLM)

* Randomly mask a portion (e.g., 15%) of input tokens
* Model predicts the masked tokens
* **Learning goal:** Deep contextual representations
* **Example:** â€œThe cat sat on the [MASK].â€ â†’ â€œmatâ€

**Why effective:**

* Enables bidirectional context (left + right)
* Core pretraining task in BERT

#### ğŸ”¸ Multi-Mask Language Modeling (MMLM)

* Variant of MLM where **multiple masks** per sequence are handled dynamically.
* Reduces overfitting to single-token predictions.
* Helps with **sentence-level understanding** (contextual reasoning).

**Insight:**
Fine-tuning on MLM or MMLM tasks can improve downstream robustness by reinforcing contextual comprehension.

---

### ğŸ”¹ 4. Fine-Tuning Strategies â€” Efficient Adaptation of Large Models

#### âš™ï¸ Gradual Unfreezing

**Problem:** Fine-tuning entire pre-trained model â†’ catastrophic forgetting.
**Solution:**

* Unfreeze layers **progressively** during training.
* Start with task-specific head â†’ deeper encoder layers â†’ lower embeddings.

**Benefits:**
âœ… Smooth adaptation
âœ… Preserves general pre-trained knowledge
âœ… Common in low-resource or domain adaptation setups

---

#### ğŸ§© Adapter Layers

**Idea:** Add **small trainable modules** (adapters) between frozen transformer layers.

* Each adapter = bottleneck MLP (down-projection â†’ non-linearity â†’ up-projection)
* Freeze base model weights; train only adapters.

**Advantages:**
âœ… Dramatically reduces fine-tuning cost
âœ… Supports **multi-task learning** (plug different adapters per task)
âœ… Faster training, less catastrophic forgetting

**Popular Implementations:**

* **AdapterFusion:** Combines multiple task adapters
* **LoRA (Low-Rank Adaptation):** Injects low-rank updates into attention weights

---

### ğŸ”¹ 5. Evaluation Frameworks â€” Measuring Model Understanding

#### ğŸ§  GLUE Benchmark (General Language Understanding Evaluation)

**Purpose:** Standardized suite for evaluating **generalization** of NLP models across tasks.

**Includes 9 core tasks:**

| Task      | Type                           | Example                                   |
| :-------- | :----------------------------- | :---------------------------------------- |
| **CoLA**  | Linguistic acceptability       | â€œIs this sentence grammatically correct?â€ |
| **SST-2** | Sentiment analysis             | â€œpositiveâ€ / â€œnegativeâ€ classification    |
| **MRPC**  | Paraphrase detection           | â€œAre these two sentences equivalent?â€     |
| **STS-B** | Semantic textual similarity    | Continuous similarity score               |
| **QQP**   | Quora Question Pairs           | Duplicate question identification         |
| **MNLI**  | Natural language inference     | Entailment / contradiction / neutral      |
| **QNLI**  | Question-answer entailment     | â€œDoes the passage answer the question?â€   |
| **RTE**   | Recognizing textual entailment | True / False inference                    |
| **WNLI**  | Coreference resolution         | â€œWho does â€˜heâ€™ refer to?â€                 |

**Metric:**

* Accuracy or F1 (classification tasks)
* Pearson/Spearman correlation (similarity tasks)

**SuperGLUE:**

* Harder extension of GLUE
* Adds commonsense reasoning and multi-sentence understanding

---

### ğŸ”¹ 6. Key Takeaways

âœ… Siamese networks â†’ learn comparable embeddings for semantic similarity
âœ… Triplet loss â†’ enforces distance structure in embedding space
âœ… MLM/MMLM â†’ pretraining objectives that enhance contextual learning
âœ… Gradual unfreezing & adapter layers â†’ stable, efficient fine-tuning
âœ… GLUE â†’ unified framework for evaluating linguistic and semantic competence

---

### ğŸ§­ Quick Mnemonics

* **â€œSiamese twins share weights.â€**
* **â€œTriplets teach distance.â€**
* **â€œMask â†’ Predict â†’ Understand.â€**
* **â€œUnfreeze slowly, forget less.â€**
* **â€œAdapters adapt, donâ€™t overwrite.â€**
* **â€œGLUE tests your true understanding.â€**

---


