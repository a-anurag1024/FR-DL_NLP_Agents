
---

### **1Ô∏è‚É£ Data Preprocessing**

**Goal:** Clean, normalize, and standardize raw text for model input.

**Key Concepts:**

* **Tokenization:** Splitting text into smaller units (words, subwords, or sentences).
* **Stopwords Removal:** Eliminating common non-informative words (e.g., *the, and, is*).
* **Stemming:** Reducing words to their root form by chopping suffixes (e.g., *playing ‚Üí play*).

  * Algorithm: **Porter Stemmer**, **Snowball Stemmer**.
  * Pros: Simple, fast; Cons: May produce non-words.
* **Lemmatization:** Mapping words to their dictionary form (lemma) using linguistic rules.

  * Uses POS tagging ‚Üí *better context awareness*.
  * Example: *went ‚Üí go*, *better ‚Üí good*.
* **Normalization:** Lowercasing, removing punctuation, expanding contractions.

**Outcome:** Clean, minimal, and uniform tokens ‚Üí ready for vectorization.

---

### **2Ô∏è‚É£ Vector Space Models & Cosine Similarity**

**Goal:** Represent words or documents as numeric vectors for comparison and computation.

**Concepts:**

* **Bag of Words (BoW):** Counts word frequencies; ignores word order.
* **TF-IDF (Term Frequency‚ÄìInverse Document Frequency):** Weighs words based on importance across documents.

  * Formula: TF √ó log(N / DF).
* **Cosine Similarity:** Measures angle similarity between two vectors.

  * Formula: cos(Œ∏) = (A ¬∑ B) / (‚ÄñA‚Äñ‚ÄñB‚Äñ).
  * Range: [-1, 1] ‚Üí 1 = identical direction.
* **Applications:**

  * Document similarity
  * Information retrieval
  * Search ranking

**Limitations:**

* High dimensionality, sparse vectors
* No semantic understanding (e.g., *car* and *automobile* appear unrelated)

---

### **3Ô∏è‚É£ Word Embedding Creation**

**Goal:** Learn dense, continuous vector representations capturing semantic meaning.

**Approaches:**

#### (a) **CBoW (Continuous Bag of Words)**

* Predict the *current word* based on surrounding *context words*.
* Example: ‚Äúthe ___ runs fast‚Äù ‚Üí predict ‚Äúdog‚Äù.
* Fast and stable for frequent words.

#### (b) **Skip-Gram**

* Predict *context words* given a *target word*.
* Works better for infrequent words.
* Example: Input = ‚Äúdog‚Äù ‚Üí Predicts context like {‚Äúthe‚Äù, ‚Äúruns‚Äù, ‚Äúfast‚Äù}.

#### (c) **Negative Sampling**

* Efficient training trick for large vocabularies.
* Instead of full softmax, only updates weights for a few ‚Äúnegative‚Äù samples per step.

#### (d) **Word2Vec**

* Implements CBoW and Skip-Gram models.
* Learns semantic & syntactic relationships.
* Captures analogies like ‚Äúking - man + woman ‚âà queen‚Äù.

#### (e) **GloVe (Global Vectors)**

* Combines global co-occurrence statistics with local context windows.
* Objective: minimize difference between predicted and actual co-occurrence ratios.
* Better at global semantic consistency.

#### (f) **FastText**

* Extends Word2Vec by representing words as n-grams (subword units).
* Helps handle **out-of-vocabulary** and **morphologically rich** languages.
* Example: ‚Äúplaying‚Äù = {‚Äúpla‚Äù, ‚Äúlay‚Äù, ‚Äúayi‚Äù, ‚Äúyin‚Äù, ‚Äúing‚Äù}.

**Outcome:** Words become dense vectors in a continuous space where distance encodes meaning.

---

### **4Ô∏è‚É£ Self-Supervised Embedding Learning & Evaluation**

**Goal:** Automatically learn word semantics without labeled data.

**Self-Supervised Tasks:**

* **Context Prediction:** (CBoW, Skip-Gram) predict missing word/context.
* **Masked Language Modeling:** predict masked token (used in BERT).
* **Next Sentence Prediction:** learn inter-sentence coherence.

**Evaluation Methods:**

* **Intrinsic Evaluation:**

  * Word similarity tasks (e.g., cosine similarity between ‚Äúking‚Äù & ‚Äúqueen‚Äù).
  * Analogy tasks (e.g., ‚ÄúParis : France :: Tokyo : ?‚Äù).
* **Extrinsic Evaluation:**

  * Downstream NLP performance (sentiment, classification, NER).

**Modern Trends:**

* Contextual embeddings (e.g., BERT, ELMo) ‚Üí depend on sentence context.
* Static embeddings (Word2Vec, GloVe) ‚Üí single vector per word.

---

**Probabilistic Modeling & Language Understanding**
---

### **1Ô∏è‚É£ N-Gram Language Models**

**Goal:** Estimate the probability of a word sequence using preceding context.

**Core Idea:**
Language is modeled as a **probability distribution over word sequences**:
P(w‚ÇÅ, w‚ÇÇ, ‚Ä¶, w‚Çô) ‚âà ‚àè P(w·µ¢ | w·µ¢‚Çã‚ÇÅ, ‚Ä¶, w·µ¢‚Çã‚Çô‚Çä‚ÇÅ)

**Concepts & Components:**

* **Unigram Model:** P(w·µ¢) ‚Äî assumes independence between words.
* **Bigram Model:** P(w·µ¢ | w·µ¢‚Çã‚ÇÅ) ‚Äî one preceding word as context.
* **Trigram Model:** P(w·µ¢ | w·µ¢‚Çã‚ÇÅ, w·µ¢‚Çã‚ÇÇ) ‚Äî two-word context for better fluency.
* **Markov Assumption:** The probability of a word depends only on the last *n‚àí1* words.
  ‚Üí Simplifies computation but loses long-term dependencies.

**Smoothing Techniques:**

* **Add-One / Laplace Smoothing:** Avoids zero probabilities by adding 1 to all counts.
* **Good-Turing Discounting:** Adjusts for unseen n-grams by redistributing probabilities.
* **Kneser-Ney Smoothing:** Advanced method using continuation probabilities for better results.

**Special Tokens:**

* **<s>** ‚Üí Start-of-sentence token.
* **</s>** ‚Üí End-of-sentence token.
  They ensure the model learns sentence boundaries and proper context windows.

**Evaluation Metric:**

* **Perplexity (PP):** Measures how well a model predicts test data.

  * Formula: PP = 2^(‚àí (1/N) Œ£ log‚ÇÇ P(w‚ÇÅ, ‚Ä¶, w‚Çô))
  * Lower perplexity ‚áí better predictive performance.

**Limitations:**

* Data sparsity (exponential growth of n-grams).
* Limited context window (short-term memory).
* Poor generalization to unseen sequences.

---

### **2Ô∏è‚É£ Naive Bayes for Text Classification**

**Goal:** Classify text (e.g., sentiment, topic) by computing class probabilities using Bayes‚Äô theorem.

**Bayes‚Äô Theorem:**
P(Class | Words) ‚àù P(Words | Class) √ó P(Class)

**Assumptions:**

* **Conditional Independence:** All features (words) are independent given the class label.

  * Example: P(good, movie | positive) = P(good | positive) √ó P(movie | positive).

**Types of Naive Bayes Models:**

* **Multinomial NB:** Works with word frequencies (most common for text).
* **Bernoulli NB:** Works with binary word presence/absence.
* **Gaussian NB:** For continuous features (less common in NLP).

**Key Components:**

* **Prior Probability:** P(Class) ‚Äî probability of each label (e.g., positive vs negative).
* **Likelihood:** P(Words | Class) ‚Äî probability of seeing the words in a document given the class.
* **Posterior Probability:** P(Class | Words) ‚Äî what we ultimately compute for classification.

**Laplacian Smoothing (Add-One):**
Prevents zero probabilities for unseen words:
P(w | c) = (Count(w, c) + 1) / (Œ£ Count(all_words, c) + V)
where *V* = vocabulary size.

**Applications:**

* **Sentiment Analysis:** Classify text as positive/negative/neutral.
* **Spam Filtering:** Classify emails as spam vs ham.
* **Topic Detection:** Assign text to categories.

**Strengths:**

* Simple, fast, and interpretable.
* Works surprisingly well on high-dimensional sparse text data.

**Limitations:**

* Independence assumption is unrealistic.
* Ignores word order and context relationships.

---

### **3Ô∏è‚É£ Parts of Speech (POS) Tagging with Markov Models**

**Goal:** Assign the most likely sequence of POS tags (e.g., Noun, Verb, Adj) to a sentence.

**Concepts:**

* Sequence labeling problem ‚Üí predict *T = {t‚ÇÅ, t‚ÇÇ, ‚Ä¶, t‚Çô}* for words *W = {w‚ÇÅ, w‚ÇÇ, ‚Ä¶, w‚Çô}*.
* Model the joint probability:
  P(W, T) = ‚àè P(w·µ¢ | t·µ¢) √ó P(t·µ¢ | t·µ¢‚Çã‚ÇÅ)

**1. Markov Approximation:**
Each tag depends only on the previous tag (first-order Markov assumption).
P(t·µ¢ | t‚ÇÅ, ‚Ä¶, t·µ¢‚Çã‚ÇÅ) ‚âà P(t·µ¢ | t·µ¢‚Çã‚ÇÅ)

**2. Hidden Markov Model (HMM):**

* **Hidden States:** POS tags (unobserved).
* **Observations:** Words (observed).
* **Parameters:**

  * Transition Probabilities ‚Üí P(t·µ¢ | t·µ¢‚Çã‚ÇÅ)
  * Emission Probabilities ‚Üí P(w·µ¢ | t·µ¢)
  * Initial Probabilities ‚Üí P(t‚ÇÅ)

**3. Viterbi Algorithm:**

* Dynamic programming algorithm to find the most likely tag sequence.
* Steps:

  1. Initialization ‚Äî Start from first word and compute probabilities.
  2. Recursion ‚Äî Propagate probabilities for each subsequent tag.
  3. Termination ‚Äî Select path with maximum probability.
* Output: Optimal tag sequence maximizing P(T | W).

**Example:**
Sentence: *He runs fast.*
‚Üí Possible tags: {PRON, VERB, ADV}
‚Üí HMM assigns probabilities and chooses most probable sequence.

**Applications:**

* Grammatical analysis
* Syntax parsing
* Preprocessing for higher-level NLP tasks

**Limitations:**

* Requires large annotated corpora for accurate transition/emission estimation.
* Struggles with long-range dependencies or ambiguous contexts.

---

**üß© Summary Flow of Section:**
Text ‚Üí Tokenized Words
‚Üì
**N-Gram Models:** Predict next word using previous ones (language modeling)
‚Üì
**Naive Bayes:** Classify text using probabilistic independence (text understanding)
‚Üì
**HMM + Viterbi:** Label sequences like POS tags using transition probabilities

---

**‚ú® Key Takeaways:**

* Probabilistic NLP relies on **conditional probability** and **independence assumptions**.
* N-Gram models capture **short-term dependencies**; HMMs capture **sequence structure**.
* Naive Bayes provides a simple but powerful **baseline for classification tasks**.
* Together, these form the **statistical foundation** upon which modern neural NLP is built.

**üß† Applied Probabilistic Techniques in NLP Tasks**

---

### **1Ô∏è‚É£ Autocorrect & Edit Distance**

**Goal:** Automatically correct spelling errors by finding the closest valid word based on minimal edit operations.

**Core Concept:**
Measure *similarity between strings* using **Edit Distance** ‚Äî the minimum number of operations required to transform one word into another.

**Key Operations:**

* **Insertion:** Add a character (e.g., ‚Äúaple‚Äù ‚Üí ‚Äúapple‚Äù)
* **Deletion:** Remove a character (e.g., ‚Äúappple‚Äù ‚Üí ‚Äúapple‚Äù)
* **Substitution:** Replace a character (e.g., ‚Äúappel‚Äù ‚Üí ‚Äúapple‚Äù)

**Levenshtein Distance:**

* Defines edit distance as the **minimum number of insertions, deletions, and substitutions**.
* Computed using **Dynamic Programming (DP)**.

**DP Recurrence Relation:**
If we denote D(i, j) as edit distance between prefixes word‚ÇÅ[0‚Ä¶i] and word‚ÇÇ[0‚Ä¶j]:

D(i, j) = min(
- D(i-1, j) + 1,         ‚Üê deletion
- D(i, j-1) + 1,         ‚Üê insertion
- D(i-1, j-1) + cost     ‚Üê substitution (cost = 0 if same char else 2)
- )

**Algorithmic Insight:**

* DP table of size m√ón where m, n are lengths of input words.
* Time complexity: O(m √ó n)

**Applications:**

* **Spell Correction:** Suggest nearest word candidates based on smallest distance.
* **Fuzzy Matching:** Find similar strings in noisy datasets.
* **OCR / Speech Correction:** Identify plausible corrections for misrecognized words.

**Probabilistic Twist:**
Autocorrect can also use *Bayesian inference*:
P(correct_word | observed_word) ‚àù P(observed_word | correct_word) √ó P(correct_word)
‚Üí Combines likelihood of typo with prior word frequency.

---

### **2Ô∏è‚É£ Machine Translation using Word Vectors**

**Goal:** Translate words or phrases between languages by aligning their semantic vector spaces.

**Key Idea:**
Words with similar meanings across languages occupy **similar positions** in vector space representations.

**Steps:**

1. **Train monolingual embeddings** separately (e.g., English Word2Vec, Spanish Word2Vec).
2. **Find linear mapping (rotation matrix)** between vector spaces.

   * Optimize for minimal distance between known translation pairs:
     minimize || W √ó x·µ¢ - y·µ¢ ||¬≤
     where x·µ¢ = source word vector, y·µ¢ = target word vector, W = transformation matrix.
3. **Orthogonal Mapping (Rotation Matrix):**

   * Ensures distances and relationships are preserved.
   * Solved via **Procrustes alignment** using Singular Value Decomposition (SVD).

**Mathematical Formulation:**
W = UV·µÄ, where X·µÄY = USV·µÄ (from SVD decomposition).

**Applications:**

* **Bilingual dictionary induction:** Automatically map words across languages.
* **Cross-lingual embeddings:** Represent multiple languages in a shared semantic space.
* **Low-resource translation:** Use transfer learning from high-resource language embeddings.

**Example:**
‚ÄúKing‚Äù in English maps near ‚ÄúRey‚Äù in Spanish after vector alignment.

**Benefits:**

* Simple, interpretable, and data-efficient.
* Enables unsupervised or weakly supervised translation.

---

### **3Ô∏è‚É£ Nearest Neighbors & Approximate Similarity Search**

**Goal:** Efficiently find the most similar words, sentences, or documents in high-dimensional vector space.

**Core Idea:**
Similarity is often defined via **distance metrics** (cosine, Euclidean).
Finding neighbors in large datasets is computationally expensive ‚Äî hence approximate methods.

**Techniques:**

**A. Nearest Neighbors (NN):**

* **Brute-force search:** Compute distance from query to all vectors.
* **Drawback:** O(N) per query ‚Äî infeasible for millions of embeddings.

**B. Locality Sensitive Hashing (LSH):**

* Hash similar items into the same ‚Äúbucket‚Äù with high probability.
* Uses random projections to reduce dimensionality while preserving similarity.
* Approximate similarity search: trade small accuracy loss for massive speed gain.

**LSH Pipeline:**

1. Generate random projection hyperplanes.
2. Compute hash for each vector (sign of dot product).
3. Compare hashes ‚Äî fewer distance computations needed.

**Applications:**

* Semantic search (find similar meanings).
* Duplicate detection.
* Large-scale recommendation systems.

**C. Approximate Nearest Neighbors (ANN):**

* Frameworks like **FAISS**, **Annoy**, **ScaNN** efficiently retrieve top-k similar vectors.
* Use tree-based or graph-based indices (e.g., HNSW).
* O(log N) retrieval instead of O(N).

**Probabilistic Perspective:**
Approximation relies on probabilistic guarantees that *similar items hash together with higher probability* than dissimilar ones.

---

**üß© Summary Flow of Section:**
Text Input
‚Üì
**Autocorrect:** Correct noisy text using edit distances and probabilistic priors
‚Üì
**Translation via Embedding Alignment:** Map semantic spaces across languages using vector transformations
‚Üì
**Similarity Search:** Retrieve or rank similar items using probabilistic hashing and approximate inference

---

**‚ú® Key Takeaways:**

* Probabilistic reasoning supports **robust real-world NLP applications** under uncertainty.
* **Edit distance** quantifies textual similarity ‚Äî foundation for correction tasks.
* **Vector alignment** bridges languages by leveraging geometric and probabilistic principles.
* **LSH & ANN** scale similarity-based NLP tasks to massive datasets efficiently.

These methods form the **practical bridge between theory and deployment**, connecting probabilistic modeling to real-world NLP systems like autocorrect, translation, and search engines.
